1) Convert COde to python
2) Get new few word vectors
3) Run tests for Skip gram , Non DISt and Global context
4) work on new FMRI dataset

EMLP *
ACL ***
NAACL **
EACL

Glove

0.499300699301  - Poor Score
0.611188811189
0.679254079254
0.675058275058
0.741724941725
0.589277389277   - Poor Score
0.62331002331
0.653613053613
0.634032634033
There are 66 words found in BrainBench from the input
The fMRI score is 0.634084

Regress done and Voxels 500
0.487179487179
0.588811188811
0.649883449883
0.662937062937
0.737995337995
0.605594405594
0.661072261072
0.673193473193
0.587412587413
There are 66 words found in BrainBench from the input
The fMRI score is 0.628231

Regress not Done and 2k Voxels

0.498834498834
0.610256410256
0.671794871795
0.679254079254
0.746386946387
0.623776223776
0.671794871795
0.682517482517
0.599533799534
There are 66 words found in BrainBench from the input
The fMRI score is 0.642683

==============================================================================
Best score : No regress, Voxels at 10% = 2K average

# of words in Glove : 66
0.498834498834
0.610256410256
0.671794871795
0.679254079254
0.746386946387
0.623776223776
0.671794871795
0.682517482517
0.599533799534
There are 66 words found in BrainBench from the input
The fMRI score for Anderson 0.642683, Old Brain Bench - 63.5
=============================================================================
Cross _ Lingual

0.507692307692
0.683916083916
0.665268065268
0.647552447552
0.725874125874
0.494172494172
0.626573426573
0.631235431235
0.638694638695
There are 66 words found in BrainBench from the input
The fMRI score for Anderson is 0.624553, Old data score: 0.63

===============================================================================

RNN

0.516657852988
0.552088841883
0.621893178213
0.725013220518
0.708090957166
0.560021152829
0.608143839238
0.639873083025
0.599153886832
There are 62 words found in BrainBench from the input
The fMRI score is 0.614548, old data score 0.62

0.555926
===============================================================================

Global Context
==============

0.528846153846 *
0.439903846154 *
0.644711538462
0.707211538462
0.687019230769
0.526923076923
0.609615384615
0.522596153846
0.564423076923
There are 65 words found in BrainBench from the input
The fMRI score is 0.581250, old data 61


=================================================================================

Non Distributional
==================

0.456876456876
0.51655011655
0.614452214452
0.637296037296
0.687645687646
0.485314685315
0.610722610723
0.568298368298
0.524941724942
There are 66 words found in BrainBench from the input
The fMRI score is 0.566900, old data :0.59

====================================================================================
Skip Gram
=========


0.534265734266
0.564568764569
0.60372960373
0.575291375291
0.662470862471
0.445221445221
0.583216783217
0.667599067599
0.58648018648
There are 66 words found in BrainBench from the input
The fMRI score is 0.580316, old data 0.65
Voxels at 0.03 and regress done ! 

The fMRI score is 0.561357

The fMRI score is 0.569697
The fMRI score is 0.581404 -0.7

The fMRI score is 0.573530

56.something
=====================================================================================



Anatomical mapping data with Datascience data
brain areas - Voxel Mapping - Tuesday
word types - concrete n discrete - Friday
open Fmri



fastText

COMMENT	FASTTEXT CREATED BY GENIUS MIKOLOV WHO MADE WORD2VEC. LEARNING IS VERY FAST! IN ORDER TO CONSIDER MORPHEMES, EACH WORD IS REPRESENTED BY THE CHARACTER NGRAM AND VECTOR EXPRESSIONS OF THEM IS LEARNED.
Year	2016
URL	https://github.com/icoxfog417/fastTextJapaneseTutorial
* it includes only in Japaneseâ€¦

Other Pre-trained Vectors
Dependency-Based Word Embeddings

COMMENT	WORD EMBEDDINGS BY LEVY ET AL. BY LEARNING DEPENDENCY-BASED CONTEXTS, IT BECAME STRONG AGAINST SYNTACTIC SIMILARITY. IT MIGHT BE GOOD IF YOU WANT TO USE IT FOR SYNTACTIC SIMILARITY.
Year	2014
URL	https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
Meta-Embeddings

COMMENT	META-EMBEDDINGS PUBLISHED IN ACL 2016. BY COMBINING DIFFERENT PUBLIC EMBEDDING SETS, BETTER VECTORS(META-EMBEDDINGS) ARE GENERATED.
Year	2016
URL	http://cistern.cis.lmu.de/meta-emb/
LexVec

COMMENT	LEXVEC ALSO PUBLISHED IN ACL 2016. IN WORD SIMILARITY TASK, SOME RESULTS EXCEED WORD2VEC.
Year	2016
URL		

https://github.com/alexandres/lexvec

